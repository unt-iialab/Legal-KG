{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entity_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzpCRDAZ58ggFx9f/vUG8U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/Legal-KG/blob/master/Legal-IE/Extraction/entity_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLZ6qFCPQtwu",
        "colab_type": "code",
        "outputId": "c59bd6b9-f658-4b17-b285-bad76f85d557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install lexnlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lexnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/69/754eae7606ed58f3d5e11f86c9ced6a0c99ecaab460e628af62d7e0e2202/lexnlp-1.4.0-py3-none-any.whl (9.7MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7MB 2.9MB/s \n",
            "\u001b[?25hCollecting datefinder-lexpredict==0.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/27/47/9a38724045b30e2e4d1c5e3e08fd3b0770dedb2e9ca92c1347b9e2182470/datefinder_lexpredict-0.6.2-py2.py3-none-any.whl\n",
            "Collecting pycountry==18.5.26\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/c0/8ce9d2b55347867900edbe4d18f790571130c16f882b4891a0f08627dcdc/pycountry-18.5.26-py2.py3-none-any.whl (10.3MB)\n",
            "\u001b[K     |████████████████████████████████| 10.3MB 41.8MB/s \n",
            "\u001b[?25hCollecting regex==2017.9.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/65/91b43adad1dc45d7374521422270490128a2f289e1c3e1036b231b521507/regex-2017.09.23.tar.gz (607kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 44.1MB/s \n",
            "\u001b[?25hCollecting reporters-db==1.0.12.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/6c/16c7c3849a25d2c3af5ef6e05d768d8e86e74aa9051df4728325c5f31f46/reporters_db-1.0.12.1-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hCollecting dateparser==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/9e/1aa87c0c59f9731820bfd20a8b148d97b315530c2c92d1fb300328c8c42f/dateparser-0.7.0-py2.py3-none-any.whl (357kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 55.2MB/s \n",
            "\u001b[?25hCollecting num2words==0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/9a/31a9151abd891ab7387d8d74cb0d84c4e77674735dbf85a63dfeb8eed6a6/num2words-0.5.7.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.5MB/s \n",
            "\u001b[?25hCollecting scipy==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n",
            "\u001b[K     |████████████████████████████████| 50.0MB 76kB/s \n",
            "\u001b[?25hCollecting typing==3.6.2\n",
            "  Downloading https://files.pythonhosted.org/packages/44/88/d09c6a7fe1af4a02f16d2f1766212bec752aadb04e5699a9706a10a1a37d/typing-3.6.2-py3-none-any.whl\n",
            "Collecting Unidecode==0.4.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/a1/9d7f3138ee3d79a1ab865a2cb38200ca778d85121db19fe264c76c981184/Unidecode-0.04.21-py2.py3-none-any.whl (228kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 42.6MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 52.2MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 39.5MB/s \n",
            "\u001b[?25hCollecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
            "\u001b[?25hCollecting us==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/72/83/8731cbf5afcf3434c0b24cfc520c11fd27bfc8a6878114662f4e3dbdab71/us-1.0.0.tar.gz\n",
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 49.8MB/s \n",
            "\u001b[?25hCollecting gensim==3.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/33/df6cb7acdcec5677ed130f4800f67509d24dbec74a03c329fcbf6b0864f0/gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
            "\u001b[K     |████████████████████████████████| 22.6MB 76.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.6/dist-packages (from datefinder-lexpredict==0.6.2->lexnlp) (2.6.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from datefinder-lexpredict==0.6.2->lexnlp) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from reporters-db==1.0.12.1->lexnlp) (1.12.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser==0.7.0->lexnlp) (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.0.0->lexnlp) (1.17.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->lexnlp) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->lexnlp) (1.24.3)\n",
            "Collecting jellyfish==0.5.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/48/ddb1458d966f0a84e472d059d87a9d1527df7768a725132fc1d810728386/jellyfish-0.5.6.tar.gz (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.4.0->lexnlp) (1.9.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.11.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim==3.4.0->lexnlp) (0.15.2)\n",
            "Building wheels for collected packages: regex, num2words, us, nltk, jellyfish\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.9.23-cp36-cp36m-linux_x86_64.whl size=538792 sha256=4c7af9df89f24a151de7275192909e28ae40b0a41156ef1b18fca55c0f884397\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/19/41/e7d239b4a53386fe9de49f9e4328799569bbeac8b8b3748876\n",
            "  Building wheel for num2words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for num2words: filename=num2words-0.5.7-cp36-none-any.whl size=84777 sha256=5665750c1bfdd07d6423308190c490cdda5b5d869e4d300c6d9c4ee943a9efc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/f6/a1/78cb080e8529bfa0c6ee2461563a4ff96efc0b6fe89cb31457\n",
            "  Building wheel for us (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for us: filename=us-1.0.0-cp36-none-any.whl size=11833 sha256=46d6dfba9cf4200690734be01ac829fbd7f4d4d1a8709d40eaf757946f85afef\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/98/40/cb8be35d7779a0ae4372c84e7a585c947bfc41540fd8999e53\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449906 sha256=364ecee576795214a49ed73914dab87b895c3a3f7e58c5d7c6b5260c4e0096c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.5.6-cp36-cp36m-linux_x86_64.whl size=74543 sha256=b9f57c85de3ce27660ae2f888aaad795b81f486fb05192a4e4bdd5b540e0e381\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/29/06/8d686d24f742cb89e7bde7f26f18cb9e89b3c8bcd6999cb12a\n",
            "Successfully built regex num2words us nltk jellyfish\n",
            "\u001b[31mERROR: xarray 0.14.1 has requirement pandas>=0.24, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.10.0 has requirement scipy>=1.0.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.25.0; python_version >= \"3.0\", but you'll have pandas 0.23.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.25 has requirement scipy>=1.1.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: regex, datefinder-lexpredict, pycountry, reporters-db, dateparser, num2words, scipy, typing, Unidecode, pandas, scikit-learn, requests, jellyfish, us, nltk, gensim, lexnlp\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: typing 3.6.6\n",
            "    Uninstalling typing-3.6.6:\n",
            "      Successfully uninstalled typing-3.6.6\n",
            "  Found existing installation: pandas 0.25.3\n",
            "    Uninstalling pandas-0.25.3:\n",
            "      Successfully uninstalled pandas-0.25.3\n",
            "  Found existing installation: scikit-learn 0.22.1\n",
            "    Uninstalling scikit-learn-0.22.1:\n",
            "      Successfully uninstalled scikit-learn-0.22.1\n",
            "  Found existing installation: requests 2.21.0\n",
            "    Uninstalling requests-2.21.0:\n",
            "      Successfully uninstalled requests-2.21.0\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed Unidecode-0.4.21 datefinder-lexpredict-0.6.2 dateparser-0.7.0 gensim-3.4.0 jellyfish-0.5.6 lexnlp-1.4.0 nltk-3.4.5 num2words-0.5.7 pandas-0.23.4 pycountry-18.5.26 regex-2017.9.23 reporters-db-1.0.12.1 requests-2.22.0 scikit-learn-0.21.3 scipy-1.0.0 typing-3.6.2 us-1.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas",
                  "requests",
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-04xUyYBQ_fa",
        "colab_type": "code",
        "outputId": "9db2bac0-ede2-42c2-9c89-be3b1ca787d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "import lexnlp.nlp.en.segments.sentences\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNywVAa-RSfj",
        "colab_type": "code",
        "outputId": "9d1d10c4-c79b-4387-a175-3694955d8b9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Split the documents into sentences\n",
        "\n",
        "path = '/content/data'\n",
        "for doc in os.listdir(path):\n",
        "  print(doc)\n",
        "  doc_path = os.path.join(path,doc)\n",
        "  with open(doc_path,'r') as file:\n",
        "   content = file.read()\n",
        "   #print(content)\n",
        "   sent = lexnlp.nlp.en.segments.sentences.get_sentence_list(content)\n",
        "   #print(sent)\n",
        "   with open('/content/process_data/'+doc,'w') as outfile:\n",
        "     for listitem in sent:\n",
        "       outfile.write('%s\\n'%listitem)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01-05-1  Adams v Tanner.txt\n",
            "01-05-2  Ex parte GWW.txt\n",
            "01-05-4  Jackson v Millspaugh.txt\n",
            "01-05-3  Graham v King.txt\n",
            "01-05-5  Jacoby v Funkhouser.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjcbg_0dS8PS",
        "colab_type": "code",
        "outputId": "67b2c236-4e18-4fbb-9c91-79fbbfa9a645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# remove some noise by defining a set of patterns\n",
        "\n",
        "path = '/content/process_data'\n",
        "for doc in os.listdir(path):\n",
        "  print(doc)\n",
        "  doc_path = os.path.join(path,doc)\n",
        "  with open(doc_path,'r') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    for i,line in enumerate(lines):\n",
        "      #print(i,line)\n",
        "      if line.startswith('v.'):\n",
        "        lines[i-1] = lines[i-1] +lines[i]\n",
        "        lines.pop(i)\n",
        "        for j, l in enumerate(lines):\n",
        "          #print(j,l)\n",
        "          if l.endswith('v.'):\n",
        "            lines[j] = lines[j] + lines[j+1]\n",
        "            lines.pop(j+1)\n",
        "          else:\n",
        "            pass\n",
        "\n",
        "      elif line.endswith('v.'):\n",
        "        lines[i] = lines[i] +lines[i+1]\n",
        "        lines.pop(i+1)\n",
        "\n",
        "      if line[:3].isdigit():\n",
        "        lines[i-1] = lines[i-1] + lines[i]\n",
        "        #print(line)\n",
        "        lines.pop(i)\n",
        "\n",
        "        for j,l in enumerate(lines):\n",
        "          if l[:3].isdigit():\n",
        "            #print(l)\n",
        "            lines[j-1] = lines[j-1] + lines[j]\n",
        "            lines.pop(j)\n",
        "          else:\n",
        "            pass\n",
        "    \n",
        "      if line.endswith(':'):\n",
        "        #print(line)\n",
        "        lines[i] = lines[i] + lines[i+1]\n",
        "        lines.pop(i+1)\n",
        "\n",
        "  with open('/content/final_clean/'+doc,'w') as outf:\n",
        "      for items in lines:\n",
        "        outf.write('%s\\n'%items)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01-05-1  Adams v Tanner.txt\n",
            "01-05-2  Ex parte GWW.txt\n",
            "01-05-4  Jackson v Millspaugh.txt\n",
            "01-05-3  Graham v King.txt\n",
            "01-05-5  Jacoby v Funkhouser.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxpl7FcUnGa",
        "colab_type": "code",
        "outputId": "87cb99e0-13b2-40c5-ba7b-b93fe4f3d432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "# entity extraction\n",
        "import pandas\n",
        "import lexnlp.extract.en.amounts\n",
        "import lexnlp.extract.en.acts\n",
        "import lexnlp.extract.en.citations\n",
        "import lexnlp.extract.en.courts\n",
        "import lexnlp.extract.en.dict_entities\n",
        "import lexnlp.extract.en.entities.nltk_re\n",
        "import lexnlp.extract.en.conditions\n",
        "import lexnlp.extract.en.copyright\n",
        "import lexnlp.extract.en.cusip\n",
        "import lexnlp.extract.en.dates\n",
        "import lexnlp.extract.en.definitions\n",
        "import lexnlp.extract.en.distances\n",
        "import lexnlp.extract.en.durations\n",
        "import lexnlp.extract.en.geoentities\n",
        "import lexnlp.extract.en.money\n",
        "import lexnlp.extract.en.percents\n",
        "import lexnlp.extract.en.pii\n",
        "import lexnlp.extract.en.ratios\n",
        "import lexnlp.extract.en.regulations\n",
        "import lexnlp.extract.en.trademarks\n",
        "import lexnlp.extract.en.urls\n",
        "import lexnlp.extract.en.entities.nltk_maxent\n",
        "import lexnlp.extract.en.entities.stanford_ner\n",
        "import lexnlp.extract.en.addresses.addresses\n",
        "\n",
        "\n",
        "court_df = pandas.read_csv(\"https://raw.githubusercontent.com/LexPredict/lexpredict-legal-dictionary/1.0.5/en/legal/us_courts.csv\")\n",
        "court_config_data = []\n",
        "for _, row in court_df.iterrows():\n",
        "    c = lexnlp.extract.en.dict_entities.entity_config(row[\"Court ID\"], row[\"Court Name\"], 0, row[\"Alias\"].split(\";\") if not pandas.isnull(row[\"Alias\"]) else [])\n",
        "    court_config_data.append(c)\n",
        "\n",
        "path = '/content/final_clean'\n",
        "for doc in os.listdir(path):\n",
        "  print(doc)\n",
        "  doc_path = os.path.join(path,doc)\n",
        "  with open(doc_path,'r') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    for line in lines:\n",
        "      # extract amount by using get_amount\n",
        "      amounts = list(lexnlp.extract.en.amounts.get_amounts(line))\n",
        "      # if len(amounts)>0:\n",
        "      #   print (amounts)\n",
        "\n",
        "      # extract acts by using get_act_list()\n",
        "      acts = lexnlp.extract.en.acts.get_act_list(line)\n",
        "      # if len(acts)>0:\n",
        "      #   print(acts)\n",
        "\n",
        "      # extract citations by using get_citations()\n",
        "      citations = list(lexnlp.extract.en.citations.get_citations(line))\n",
        "      if len(citations)>0:\n",
        "        print(citations)\n",
        "\n",
        "      # extract courts by using get_courts()\n",
        "      for entity, alias in lexnlp.extract.en.courts.get_courts(line, court_config_data):\n",
        "        print(\"entity=\", entity)\n",
        "        print(\"alias=\", alias)\n",
        "      \n",
        "      # extract companies by using nltk_re.get_entities.nltk_re.get_companies()\n",
        "      companies = list(lexnlp.extract.en.entities.nltk_re.get_entities.nltk_re.get_companies(line))\n",
        "      # companies = list(lexnlp.extract.en.entities.nltk_re.get_entities.nltk_re(text))\n",
        "\n",
        "      # extract conditions by using get_conditions()\n",
        "      conditions = list(lexnlp.extract.en.conditions.get_conditions(line))\n",
        "\n",
        "      # extract constraints by using get_constraints()\n",
        "      constraints = list(lexnlp.extract.en.constraints.get_constraints(line))\n",
        "\n",
        "      # extract copyright by using get_copyright()\n",
        "      copyrights = list(lexnlp.extract.en.copyright.get_copyright(line))\n",
        "\n",
        "      # extract cusip by using get_cusip()\n",
        "      cusips = lexnlp.extract.en.cusip.get_cusip(line)\n",
        "\n",
        "      # extract dates by using get_dates()\n",
        "      dates = list(lexnlp.extract.en.dates.get_dates(line))\n",
        "\n",
        "      # extract definitions by using get_definitions()\n",
        "      definitions = list(lexnlp.extract.en.definitions.get_definitions(line))\n",
        "\n",
        "      # extract distances by using get_distances()\n",
        "      distances = list(lexnlp.extract.en.distances.get_distances(line))\n",
        "\n",
        "      # extract durations by using get_durations()\n",
        "      durations = list(lexnlp.extract.en.durations.get_durations(line))\n",
        "\n",
        "      # extract geoentities by using get_geoentities()\n",
        "      geoentities = lexnlp.extract.en.geoentities.get_geoentities(line)\n",
        "\n",
        "\n",
        "      # extract money by using get_money()\n",
        "      moneys = list(lexnlp.extract.en.money.get_money(line))\n",
        "\n",
        "      # extract percents by using .get_percents()\n",
        "      percents = list(lexnlp.extract.en.percents.get_percents(line))\n",
        "\n",
        "      # extract all the piis by using get_pii()\n",
        "      piis = list(lexnlp.extract.en.pii.get_pii(line))\n",
        "\n",
        "      # extract all the ratios by using get_ratios()\n",
        "      ratios = list(lexnlp.extract.en.ratios.get_ratios(line))\n",
        "\n",
        "      # extract all the regulations by using get_regulations() \n",
        "      regulations = list(lexnlp.extract.en.regulations.get_regulations(line))\n",
        "\n",
        "      # extract all the trademarks by using get_trademarks()\n",
        "      trademarks = list(lexnlp.extract.en.trademarks.get_trademarks(line))\n",
        "\n",
        "      # extract all the urls by using get_urls()\n",
        "      urls = list(lexnlp.extract.en.urls.get_urls(line))\n",
        "\n",
        "      # extract all the persons by using entities.nltk_maxent.get_persons()\n",
        "      persons = list(lexnlp.extract.en.entities.nltk_maxent.get_persons(line))\n",
        "      # persons = list(lexnlp.extract.en.entities.stanford_ner.get_persons(line))\n",
        "\n",
        "      # extract all the geopolitical by using entities.nltk_maxent.get_geopolitical()\n",
        "      geopolitical = list(lexnlp.extract.en.entities.nltk_maxent.get_geopolitical(line))\n",
        "\n",
        "      # extract noun_phrases\n",
        "      noun_phrases = list(lexnlp.extract.en.entities.nltk_maxent.get_noun_phrases(line))\n",
        "\n",
        "      # extract organizations\n",
        "      organizations = list(lexnlp.extract.en.entities.stanford_ner.get_organizations(line))\n",
        "\n",
        "      # extract locations\n",
        "      locations = list(lexnlp.extract.en.entities.stanford_ner.get_locations(line))\n",
        "\n",
        "      # extract address \n",
        "      addresses = list(lexnlp.extract.en.addresses.addresses.get_addresses(line))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01-05-1  Adams v Tanner.txt\n",
            "entity= (1, 'United States Supreme Court', 0, [('United States Supreme Court', None, False, None, ' united states supreme court '), ('Supreme Court', None, False, None, ' supreme court '), (' SCOTUS', None, False, None, ' scotus ')])\n",
            "alias= ('Supreme Court', None, False, None, ' supreme court ')\n",
            "[(1843, 'WL', 'West Law Citation', 284, None, None, None)]\n",
            "[(8, 'Cal.', 'California Reports', 145, '147', None, None)]\n",
            "01-05-2  Ex parte GWW.txt\n",
            "entity= (1, 'United States Supreme Court', 0, [('United States Supreme Court', None, False, None, ' united states supreme court '), ('Supreme Court', None, False, None, ' supreme court '), (' SCOTUS', None, False, None, ' scotus ')])\n",
            "alias= ('Supreme Court', None, False, None, ' supreme court ')\n",
            "[(2014, 'WL', 'West Law Citation', 11352677, None, None, None)]\n",
            "entity= (1, 'United States Supreme Court', 0, [('United States Supreme Court', None, False, None, ' united states supreme court '), ('Supreme Court', None, False, None, ' supreme court '), (' SCOTUS', None, False, None, ' scotus ')])\n",
            "alias= ('Supreme Court', None, False, None, ' supreme court ')\n",
            "01-05-4  Jackson v Millspaugh.txt\n",
            "entity= (1, 'United States Supreme Court', 0, [('United States Supreme Court', None, False, None, ' united states supreme court '), ('Supreme Court', None, False, None, ' supreme court '), (' SCOTUS', None, False, None, ' scotus ')])\n",
            "alias= ('Supreme Court', None, False, None, ' supreme court ')\n",
            "[(78, 'N.W.', 'North Western Reporter', 936, '937', None, None)]\n",
            "01-05-3  Graham v King.txt\n",
            "entity= (1, 'United States Supreme Court', 0, [('United States Supreme Court', None, False, None, ' united states supreme court '), ('Supreme Court', None, False, None, ' supreme court '), (' SCOTUS', None, False, None, ' scotus ')])\n",
            "alias= ('Supreme Court', None, False, None, ' supreme court ')\n",
            "[(1849, 'WL', 'West Law Citation', 301, None, None, None)]\n",
            "[(50, 'Cal.', 'California Reports', 145, '148', None, None)]\n",
            "[(55, 'Tex.', 'Texas Reports', 563, '570', None, None)]\n",
            "01-05-5  Jacoby v Funkhouser.txt\n",
            "entity= (1, 'United States Supreme Court', 0, [('United States Supreme Court', None, False, None, ' united states supreme court '), ('Supreme Court', None, False, None, ' supreme court '), (' SCOTUS', None, False, None, ' scotus ')])\n",
            "alias= ('Supreme Court', None, False, None, ' supreme court ')\n",
            "entity= (17, 'Southern District of Alabama', 0, [('Southern District of Alabama', None, False, None, ' southern district of alabama '), ('S.D. Ala.', None, False, None, ' s . d . ala . ')])\n",
            "alias= ('S.D. Ala.', None, False, None, ' s . d . ala . ')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}